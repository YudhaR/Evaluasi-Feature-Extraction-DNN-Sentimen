{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M_V4Ud4_jxTO"
      },
      "outputs": [],
      "source": [
        "#Yudha Rizqia Grafer//@ydhrizqi._\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BI-hQADqjxTP"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 5000\n",
        "MAX_NUM_WORDS = 25000\n",
        "EMBEDDING_DIM = 64\n",
        "TEST_SPLIT = 0.3\n",
        "\n",
        "TEXT_DATA = 'dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ybJ-TLPCjxTQ"
      },
      "outputs": [],
      "source": [
        "# define a function that allows us to evaluate our models\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_model(predict_fun, X_train, y_train, X_test, y_test):\n",
        "    '''\n",
        "    evaluate the model, both training and testing errors are reported\n",
        "    '''\n",
        "    # training error\n",
        "    y_predict_train = predict_fun(X_train)\n",
        "    train_acc = accuracy_score(y_train,y_predict_train)\n",
        "\n",
        "    # testing error\n",
        "    y_predict_test = predict_fun(X_test)\n",
        "    test_acc = accuracy_score(y_test,y_predict_test)\n",
        "\n",
        "    return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TZ24waD-jxTQ"
      },
      "outputs": [],
      "source": [
        "# estimate 95% confidence interval on error\n",
        "\n",
        "# NOTE: based on conversation on stackexchange:\n",
        "# https://stats.stackexchange.com/questions/247551/how-to-determine-the-confidence-of-a-neural-network-prediction\n",
        "# towards bottom of the page.\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "def error_conf(error, n):\n",
        "    term = 1.96*sqrt((error*(1-error))/n)\n",
        "    lb = error - term\n",
        "    ub = error + term\n",
        "\n",
        "    return lb, ub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fjCaUesjxTQ"
      },
      "outputs": [],
      "source": [
        "# # read in our data and preprocess it\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "# df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "# # only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]\n",
        "\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# # Filter out rows where 'cleaned_text' is an empty string\n",
        "# mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "# df = df[mask]\n",
        "\n",
        "# # Buat mask untuk memilih teks yang bukan NaN dan memiliki panjang lebih dari 0\n",
        "# mask = df['Text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "\n",
        "# # Terapkan mask untuk menghapus baris yang kosong\n",
        "# df_cleaned = df[mask].copy()\n",
        "\n",
        "# # Hapus nilai di kolom lain pada baris yang sama\n",
        "# df_cleaned.loc[~mask, :] = None\n",
        "\n",
        "# # Reset index setelah menghapus baris\n",
        "# df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# df = df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7QN0yFI9r-I",
        "outputId": "00b43daf-f5ca-4fd3-e81c-0f748d68b943"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data cleaning and augmentation completed and saved to 'augmented_dataset.csv'.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import random\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "# Synonym Replacement (SR)\n",
        "def synonym_replacement(words, n):\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return new_words\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "            synonyms.add(synonym)\n",
        "    return list(synonyms)\n",
        "\n",
        "# Random Insertion (RI)\n",
        "def random_insertion(words, n):\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        add_word(new_words)\n",
        "    return new_words\n",
        "\n",
        "def add_word(new_words):\n",
        "    synonyms = []\n",
        "    counter = 0\n",
        "    while len(synonyms) < 1:\n",
        "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        counter += 1\n",
        "        if counter >= 10:\n",
        "            return\n",
        "    random_synonym = synonyms[0]\n",
        "    random_idx = random.randint(0, len(new_words)-1)\n",
        "    new_words.insert(random_idx, random_synonym)\n",
        "\n",
        "# Random Swap (RS)\n",
        "def random_swap(words, n):\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        new_words = swap_word(new_words)\n",
        "    return new_words\n",
        "\n",
        "def swap_word(new_words):\n",
        "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
        "    random_idx_2 = random_idx_1\n",
        "    counter = 0\n",
        "    while random_idx_2 == random_idx_1:\n",
        "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
        "        counter += 1\n",
        "        if counter > 3:\n",
        "            return new_words\n",
        "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
        "    return new_words\n",
        "\n",
        "# Random Deletion (RD)\n",
        "def random_deletion(words, p):\n",
        "    if len(words) == 1:\n",
        "        return words\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        r = random.uniform(0, 1)\n",
        "        if r > p:\n",
        "            new_words.append(word)\n",
        "    if len(new_words) == 0:\n",
        "        rand_int = random.randint(0, len(words)-1)\n",
        "        return [words[rand_int]]\n",
        "    return new_words\n",
        "\n",
        "# Apply EDA to each row in the 'text' column\n",
        "def apply_eda(text):\n",
        "    words = text.split()\n",
        "    augmented_texts = []\n",
        "    num_augmented_texts = 4  # Number of augmented texts to generate\n",
        "\n",
        "    for _ in range(num_augmented_texts):\n",
        "        operation = random.choice(['sr', 'ri', 'rs', 'rd'])\n",
        "        if operation == 'sr':\n",
        "            augmented_text = synonym_replacement(words, 1)\n",
        "        elif operation == 'ri':\n",
        "            augmented_text = random_insertion(words, 1)\n",
        "        elif operation == 'rs':\n",
        "            augmented_text = random_swap(words, 1)\n",
        "        elif operation == 'rd':\n",
        "            augmented_text = random_deletion(words, 0.1)\n",
        "        augmented_texts.append(' '.join(augmented_text))\n",
        "\n",
        "    return augmented_texts\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove non-alphabetical characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Join words back to a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply EDA and expand the DataFrame\n",
        "augmented_rows = []\n",
        "for _, row in df.iterrows():\n",
        "    augmented_texts = apply_eda(row['text'])\n",
        "    for text in augmented_texts:\n",
        "        augmented_rows.append({'text': text, 'label': row['label']})\n",
        "\n",
        "augmented_df = pd.DataFrame(augmented_rows)\n",
        "\n",
        "# Clean the augmented data\n",
        "augmented_df['cleaned_text'] = augmented_df['text'].apply(clean_text)\n",
        "\n",
        "# Clean the original data\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Combine original and augmented data\n",
        "combined_df = pd.concat([df[['cleaned_text', 'label']], augmented_df[['cleaned_text', 'label']]], ignore_index=True)\n",
        "\n",
        "# Save the cleaned and augmented DataFrame to a new CSV file\n",
        "combined_df.to_csv('augmented_dataset.csv', index=False)\n",
        "\n",
        "print(\"Data cleaning and augmentation completed and saved to 'augmented_dataset.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWSsj4Hz-vQa"
      },
      "outputs": [],
      "source": [
        "TEXT_DATA = 'augmented_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "df = df[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOZR1rhZjxTR",
        "outputId": "881a794c-d6b0-4c92-a318-96bcdd6d85db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34565 texts.\n"
          ]
        }
      ],
      "source": [
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['cleaned_text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "mJZQaPZMjxTR",
        "outputId": "7b38cc6c-e019-49c7-baac-945d3ead8c23"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the distribution of article lengths in terms of word counts\n",
        "\n",
        "text_lengths = texts.apply(lambda x: len(x.split(\" \")))\n",
        "plt.hist(text_lengths)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFPHg-0NBBso"
      },
      "source": [
        "## Lexicon Based with EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMDZL4VbjxTW",
        "outputId": "fc3d2f01-d823-4240-99f9-f176d2be8f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34572 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'augmented_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "df = df[mask]\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['cleaned_text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOD1T6sxBJsV",
        "outputId": "5e8cb633-0fb4-4c40-c4a7-193c11eadbbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fungsi untuk mendapatkan skor sentimen VADER dan mengubah label\n",
        "def get_vader_sentiment(text):\n",
        "    sentiment = sid.polarity_scores(text)\n",
        "    # Jika compound > 0, label positive; jika <= 0, label negative\n",
        "    label = 1 if sentiment['compound'] > 0 else 0\n",
        "    return sentiment['neg'], sentiment['neu'], sentiment['pos'], sentiment['compound'], label\n",
        "\n",
        "# Terapkan fungsi ke setiap teks di list\n",
        "lexicon_features = [get_vader_sentiment(text) for text in texts]\n",
        "\n",
        "# Buat DataFrame dari hasil ekstraksi fitur leksikon\n",
        "df_lexicon = pd.DataFrame(lexicon_features, columns=['neg', 'neu', 'pos', 'compound', 'label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rmuq_ufIgvF",
        "outputId": "ea45b73e-d665-4caf-d761-65f3adbfe7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        1\n",
            "4        0\n",
            "        ..\n",
            "34567    0\n",
            "34568    0\n",
            "34569    0\n",
            "34570    0\n",
            "34571    0\n",
            "Name: label, Length: 34572, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df_lexicon['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEVjurZMD_53",
        "outputId": "5bef9e48-05f2-452a-bbe3-a985930a9936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11011\n",
            "Found 11010 unique tokens.\n",
            "Shape of data tensor: (34572, 5000)\n",
            "Shape of label tensor: (34572,)\n"
          ]
        }
      ],
      "source": [
        "labels = df_lexicon['label']\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "data = pad_sequences(sequences,\n",
        "                     maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                     padding='pre',\n",
        "                     truncating='pre')\n",
        "print(num_words)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33Wt6h7TI3TH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(data,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtLZlObQI77u",
        "outputId": "5dbf113b-0a06-4453-c2d6-c65147003795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5000, 64)          704704    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 5000, 128)         66048     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5000, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 813289 (3.10 MB)\n",
            "Trainable params: 813289 (3.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# build a 1D convnet with global maxpooling\n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM,\n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Bidirectional(LSTM(32)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJl5VIQmJDKG",
        "outputId": "2ccc248d-0ced-42ee-d1f8-e8febf641943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "757/757 [==============================] - 483s 625ms/step - loss: 0.2826 - accuracy: 0.8738 - val_loss: 0.1316 - val_accuracy: 0.9590\n",
            "Epoch 2/10\n",
            "757/757 [==============================] - 451s 596ms/step - loss: 0.0766 - accuracy: 0.9779 - val_loss: 0.0906 - val_accuracy: 0.9741\n",
            "Epoch 3/10\n",
            "757/757 [==============================] - 446s 590ms/step - loss: 0.0442 - accuracy: 0.9867 - val_loss: 0.0738 - val_accuracy: 0.9781\n",
            "Epoch 4/10\n",
            "757/757 [==============================] - 448s 591ms/step - loss: 0.0264 - accuracy: 0.9919 - val_loss: 0.0784 - val_accuracy: 0.9813\n",
            "Epoch 5/10\n",
            "757/757 [==============================] - 446s 590ms/step - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.0759 - val_accuracy: 0.9816\n",
            "Epoch 6/10\n",
            "757/757 [==============================] - 448s 592ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 0.0935 - val_accuracy: 0.9819\n",
            "Epoch 7/10\n",
            "757/757 [==============================] - 448s 591ms/step - loss: 0.0113 - accuracy: 0.9959 - val_loss: 0.0884 - val_accuracy: 0.9832\n",
            "Epoch 8/10\n",
            "757/757 [==============================] - 447s 590ms/step - loss: 0.0121 - accuracy: 0.9955 - val_loss: 0.0912 - val_accuracy: 0.9835\n",
            "Epoch 9/10\n",
            "757/757 [==============================] - 451s 595ms/step - loss: 0.0104 - accuracy: 0.9963 - val_loss: 0.1389 - val_accuracy: 0.9691\n",
            "Epoch 10/10\n",
            "757/757 [==============================] - 450s 594ms/step - loss: 0.0110 - accuracy: 0.9960 - val_loss: 0.1064 - val_accuracy: 0.9845\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J--9I26F8uqF",
        "outputId": "b8115f48-acff-4235-92db-824bc5fbde27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 71s 219ms/step\n",
            "Persentase prediksi benar: 50.06%\n"
          ]
        }
      ],
      "source": [
        "# # Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()  # Mengonversi probabilitas menjadi label biner dan memastikannya 1D\n",
        "\n",
        "# Ambil indeks dari x_val\n",
        "val_indices = x_val.index if hasattr(x_val, 'index') else range(len(x_val))\n",
        "\n",
        "# Pastikan labels memiliki indeks yang benar\n",
        "labels = df['label']\n",
        "\n",
        "# Buat DataFrame perbandingan\n",
        "comparison_df = pd.DataFrame({'true_label': labels.iloc[val_indices].values, 'pred_label': y_pred})\n",
        "\n",
        "# Menghitung persentase prediksi benar\n",
        "correct_predictions = (comparison_df['true_label'] == comparison_df['pred_label']).sum()\n",
        "total_predictions = len(comparison_df)\n",
        "accuracy = correct_predictions / total_predictions * 100\n",
        "\n",
        "print(f\"Persentase prediksi benar: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lExjL2PbMStw"
      },
      "source": [
        "## CV and TF_IDF, Bigram with EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDWQlhZSNWzw",
        "outputId": "c465500d-ac18-4a75-9852-6b86526a5437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34572 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'augmented_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "df = df[mask]\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['cleaned_text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ocX99c7O_pY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, max_df=0.95, min_df=2, ngram_range=(1, 2))\n",
        "\n",
        "# Transformasi teks menjadi fitur TF-IDF\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_tfidf,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGQKfLqxz8Hh",
        "outputId": "9eb286c5-4463-497c-a3fd-1843c73390df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(X_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpu_r0bkPQp0",
        "outputId": "e86fa72a-a8d7-4942-decb-5f3b93c33cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5000, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 5000, 128)         66048     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5000, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428585 (1.63 MB)\n",
            "Trainable params: 428585 (1.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "num_words = X_tfidf.shape[1]  # Jumlah kata setara dengan fitur dari TF-IDF\n",
        "\n",
        "# Definisi model LSTM\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM,\n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Bidirectional(LSTM(32)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6q3QfsqQGAD",
        "outputId": "95f808d6-34bf-41fb-8c20-4d8c6c2b82d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "757/757 [==============================] - 600s 778ms/step - loss: 0.6931 - accuracy: 0.5158 - val_loss: 0.6933 - val_accuracy: 0.5230\n",
            "Epoch 2/10\n",
            "757/757 [==============================] - 482s 637ms/step - loss: 0.6926 - accuracy: 0.5195 - val_loss: 0.6923 - val_accuracy: 0.5230\n",
            "Epoch 3/10\n",
            "757/757 [==============================] - 474s 626ms/step - loss: 0.6925 - accuracy: 0.5202 - val_loss: 0.6921 - val_accuracy: 0.5230\n",
            "Epoch 4/10\n",
            "757/757 [==============================] - 474s 626ms/step - loss: 0.6924 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5230\n",
            "Epoch 5/10\n",
            "757/757 [==============================] - 477s 630ms/step - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5230\n",
            "Epoch 6/10\n",
            "757/757 [==============================] - 468s 619ms/step - loss: 0.6924 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5230\n",
            "Epoch 7/10\n",
            "757/757 [==============================] - 474s 627ms/step - loss: 0.6923 - accuracy: 0.5210 - val_loss: 0.6922 - val_accuracy: 0.5230\n",
            "Epoch 8/10\n",
            "757/757 [==============================] - 473s 625ms/step - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5230\n",
            "Epoch 9/10\n",
            "757/757 [==============================] - 469s 619ms/step - loss: 0.6924 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5230\n",
            "Epoch 10/10\n",
            "757/757 [==============================] - 466s 615ms/step - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6921 - val_accuracy: 0.5230\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-qoa9dUz4Bq",
        "outputId": "31ffbcd3-cd89-4c75-8e65-ad118994a2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 78s 236ms/step\n",
            "Validation Accuracy: 0.5230\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmrYS6TY-Gd5"
      },
      "source": [
        "## CV dan TF-IDF, Tri-Gram with EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4jAUeWV-QHb",
        "outputId": "b93103a8-299b-47e3-c5d5-ee3495955ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34572 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'augmented_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "df = df[mask]\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['cleaned_text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkpmPegu-kdb"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, max_df=0.95, min_df=2, ngram_range=(1, 3))\n",
        "\n",
        "# Transformasi teks menjadi fitur TF-IDF\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_tfidf,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF_MzR2M-rvn",
        "outputId": "cdced0e8-5720-4563-df7d-8f18b8e08003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5000, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 5000, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 5000, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428585 (1.63 MB)\n",
            "Trainable params: 428585 (1.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "num_words = X_tfidf.shape[1]  # Jumlah kata setara dengan fitur dari TF-IDF\n",
        "\n",
        "# Definisi model LSTM\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM,\n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Bidirectional(LSTM(32)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er3Rlmt0-u1Z",
        "outputId": "98cc2932-774f-4a86-b991-334d7aedc743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "757/757 [==============================] - 457s 594ms/step - loss: 0.6931 - accuracy: 0.5187 - val_loss: 0.6924 - val_accuracy: 0.5212\n",
            "Epoch 2/10\n",
            "757/757 [==============================] - 446s 589ms/step - loss: 0.6926 - accuracy: 0.5215 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 3/10\n",
            "757/757 [==============================] - 448s 591ms/step - loss: 0.6924 - accuracy: 0.5210 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 4/10\n",
            "757/757 [==============================] - 441s 583ms/step - loss: 0.6922 - accuracy: 0.5219 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 5/10\n",
            "757/757 [==============================] - 464s 613ms/step - loss: 0.6925 - accuracy: 0.5221 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 6/10\n",
            "757/757 [==============================] - 468s 619ms/step - loss: 0.6923 - accuracy: 0.5216 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 7/10\n",
            "757/757 [==============================] - 473s 625ms/step - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 8/10\n",
            "757/757 [==============================] - 473s 625ms/step - loss: 0.6923 - accuracy: 0.5219 - val_loss: 0.6922 - val_accuracy: 0.5212\n",
            "Epoch 9/10\n",
            "757/757 [==============================] - 462s 611ms/step - loss: 0.6923 - accuracy: 0.5219 - val_loss: 0.6923 - val_accuracy: 0.5212\n",
            "Epoch 10/10\n",
            "757/757 [==============================] - 466s 615ms/step - loss: 0.6923 - accuracy: 0.5219 - val_loss: 0.6923 - val_accuracy: 0.5212\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyxwNnvo-xBz",
        "outputId": "c1206029-f38c-449d-af6a-a3b8574ac57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 79s 238ms/step\n",
            "Validation Accuracy: 0.5212\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dOgxNOm6K1b"
      },
      "source": [
        "## Word2Vec DNN with EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUhF7Eo66PcJ",
        "outputId": "b74d6430-1ed1-4b62-e8cb-ba6f3d6786cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34572 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'augmented_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "df = df[mask]\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['cleaned_text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsUUPqbI6Sji"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-tl1SAC6T7A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def text_to_vector(text, model):\n",
        "    words = text.split()\n",
        "    word_vectors = [model_w2v[word] for word in words if word in model_w2v]\n",
        "    if len(word_vectors) > 0:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(300)\n",
        "\n",
        "# Transformasi teks menjadi vektor Word2Vec\n",
        "X_word2vec = np.array([text_to_vector(text, model) for text in texts])\n",
        "\n",
        "# Reshape input to include timestep dimension\n",
        "X_word2vec = np.expand_dims(X_word2vec, axis=1)  # Shape will be (samples, 1, 300)\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_word2vec,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBf2S0C3-QeK",
        "outputId": "37659223-b690-4c7e-80a5-49a177faa259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.10950089  0.06551361  0.03105164 ... -0.06450653 -0.04360962\n",
            "   0.03709412]\n",
            " [-0.01240412  0.0085729  -0.03882344 ...  0.03937276 -0.05004501\n",
            "   0.05314128]\n",
            " [ 0.09446716  0.04914856  0.04529095 ... -0.03196645 -0.0065155\n",
            "   0.04518127]\n",
            " ...\n",
            " [-0.07632446  0.00054932  0.04919434 ... -0.02539062 -0.04977417\n",
            "  -0.0043335 ]\n",
            " [-0.07632446  0.00054932  0.04919434 ... -0.02539062 -0.04977417\n",
            "  -0.0043335 ]\n",
            " [-0.02396647 -0.04125977  0.04410807 ... -0.1110026  -0.03039551\n",
            "  -0.0715332 ]]\n"
          ]
        }
      ],
      "source": [
        "print(X_word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctpcc0OZ6Vbw",
        "outputId": "c17c5ab3-7cb6-4ce2-b00d-7bf217fbc9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_22 (Bidirect  (None, 1, 128)            186880    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_23 (Bidirect  (None, 64)                41216     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 229417 (896.16 KB)\n",
            "Trainable params: 229417 (896.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(1, 300))),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build model\n",
        "model.build(input_shape=(None, 1, 300))\n",
        "\n",
        "# Build model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qhn7DJq6cpa",
        "outputId": "7f3e2a48-e553-439a-b5f8-54dec1a394ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "757/757 [==============================] - 20s 12ms/step - loss: 0.5632 - accuracy: 0.7067 - val_loss: 0.5128 - val_accuracy: 0.7519\n",
            "Epoch 2/10\n",
            "757/757 [==============================] - 8s 11ms/step - loss: 0.5119 - accuracy: 0.7471 - val_loss: 0.4976 - val_accuracy: 0.7573\n",
            "Epoch 3/10\n",
            "757/757 [==============================] - 8s 10ms/step - loss: 0.4956 - accuracy: 0.7557 - val_loss: 0.4870 - val_accuracy: 0.7634\n",
            "Epoch 4/10\n",
            "757/757 [==============================] - 8s 11ms/step - loss: 0.4851 - accuracy: 0.7606 - val_loss: 0.4843 - val_accuracy: 0.7707\n",
            "Epoch 5/10\n",
            "757/757 [==============================] - 8s 11ms/step - loss: 0.4731 - accuracy: 0.7677 - val_loss: 0.4752 - val_accuracy: 0.7724\n",
            "Epoch 6/10\n",
            "757/757 [==============================] - 7s 10ms/step - loss: 0.4621 - accuracy: 0.7723 - val_loss: 0.4601 - val_accuracy: 0.7804\n",
            "Epoch 7/10\n",
            "757/757 [==============================] - 8s 10ms/step - loss: 0.4506 - accuracy: 0.7774 - val_loss: 0.4494 - val_accuracy: 0.7844\n",
            "Epoch 8/10\n",
            "757/757 [==============================] - 7s 10ms/step - loss: 0.4393 - accuracy: 0.7823 - val_loss: 0.4364 - val_accuracy: 0.7922\n",
            "Epoch 9/10\n",
            "757/757 [==============================] - 8s 10ms/step - loss: 0.4233 - accuracy: 0.7951 - val_loss: 0.4348 - val_accuracy: 0.7944\n",
            "Epoch 10/10\n",
            "757/757 [==============================] - 8s 10ms/step - loss: 0.4092 - accuracy: 0.8023 - val_loss: 0.4058 - val_accuracy: 0.8124\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq-kX5fO8uGs",
        "outputId": "586d480d-b632-4e95-9e02-fe582bd9804f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 2s 3ms/step\n",
            "Validation Accuracy: 0.8124\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY-wM9Wdrb0z"
      },
      "source": [
        "## FastText DNN with EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71kD1YUAre2Y",
        "outputId": "649f766b-938b-44ba-bce2-03510f472239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 34572 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'augmented_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "mask = df['cleaned_text'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
        "df = df[mask]\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['cleaned_text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK1OokJBrh5z",
        "outputId": "b79e6395-ad6c-4f91-dd92-e34f5522493f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the FastText model\n",
        "model_fst = api.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg3khBEfrnXj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def text_to_vector(text, model):\n",
        "    words = text.split()\n",
        "    word_vectors = [model_fst[word] for word in words if word in model_fst]\n",
        "    if len(word_vectors) > 0:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(300)\n",
        "\n",
        "# Transformasi teks menjadi vektor Word2Vec\n",
        "X_fastT = np.array([text_to_vector(text, model_fst) for text in texts])\n",
        "\n",
        "# Reshape input to include timestep dimension\n",
        "X_fastT = np.expand_dims(X_fastT, axis=1)  # Shape will be (samples, 1, 300)\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_fastT,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWo4yovtr0QK",
        "outputId": "980df836-be94-4c08-b8a9-58ae00f90950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[-0.016029   -0.02077311  0.01020111 ...  0.03221222  0.02648128\n",
            "    0.00495643]]\n",
            "\n",
            " [[ 0.00516947 -0.0215895   0.00109633 ...  0.01404043  0.02546367\n",
            "    0.0166717 ]]\n",
            "\n",
            " [[-0.02298148 -0.04577361  0.02517751 ...  0.01859763  0.02254397\n",
            "   -0.01706745]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.00011492 -0.0147848   0.02251914 ... -0.01200712  0.0170596\n",
            "   -0.02479552]]\n",
            "\n",
            " [[ 0.00011492 -0.0147848   0.02251914 ... -0.01200712  0.0170596\n",
            "   -0.02479552]]\n",
            "\n",
            " [[-0.00385868 -0.0041866   0.02648598 ... -0.01238012  0.01481294\n",
            "   -0.03089488]]]\n"
          ]
        }
      ],
      "source": [
        "print(X_fastT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Y-EBeisAUZ",
        "outputId": "463fba8a-f143-4d72-ffa4-87f9f94267fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 1, 128)            186880    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 229417 (896.16 KB)\n",
            "Trainable params: 229417 (896.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(1, 300))),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build model\n",
        "model.build(input_shape=(None, 1, 300))\n",
        "\n",
        "# Build model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJqRtsQjsB1D",
        "outputId": "59988cd9-9b14-4391-a50e-f97b3f7f94e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "757/757 [==============================] - 25s 17ms/step - loss: 0.5865 - accuracy: 0.6842 - val_loss: 0.5271 - val_accuracy: 0.7398\n",
            "Epoch 2/10\n",
            "757/757 [==============================] - 10s 14ms/step - loss: 0.5315 - accuracy: 0.7369 - val_loss: 0.5153 - val_accuracy: 0.7487\n",
            "Epoch 3/10\n",
            "757/757 [==============================] - 11s 14ms/step - loss: 0.5197 - accuracy: 0.7441 - val_loss: 0.5154 - val_accuracy: 0.7441\n",
            "Epoch 4/10\n",
            "757/757 [==============================] - 11s 15ms/step - loss: 0.5101 - accuracy: 0.7477 - val_loss: 0.5030 - val_accuracy: 0.7499\n",
            "Epoch 5/10\n",
            "757/757 [==============================] - 12s 15ms/step - loss: 0.5045 - accuracy: 0.7500 - val_loss: 0.4998 - val_accuracy: 0.7529\n",
            "Epoch 6/10\n",
            "757/757 [==============================] - 10s 14ms/step - loss: 0.4966 - accuracy: 0.7536 - val_loss: 0.4931 - val_accuracy: 0.7558\n",
            "Epoch 7/10\n",
            "757/757 [==============================] - 11s 15ms/step - loss: 0.4951 - accuracy: 0.7548 - val_loss: 0.4931 - val_accuracy: 0.7589\n",
            "Epoch 8/10\n",
            "757/757 [==============================] - 10s 14ms/step - loss: 0.4901 - accuracy: 0.7556 - val_loss: 0.4918 - val_accuracy: 0.7568\n",
            "Epoch 9/10\n",
            "757/757 [==============================] - 11s 14ms/step - loss: 0.4859 - accuracy: 0.7540 - val_loss: 0.4837 - val_accuracy: 0.7590\n",
            "Epoch 10/10\n",
            "757/757 [==============================] - 10s 13ms/step - loss: 0.4817 - accuracy: 0.7574 - val_loss: 0.4839 - val_accuracy: 0.7622\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NKQxdEGsDDI",
        "outputId": "9221c49f-6aec-4c0f-f0f1-b27a6e57922e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "325/325 [==============================] - 3s 5ms/step\n",
            "Validation Accuracy: 0.7622\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMzl-kxJlk69"
      },
      "source": [
        "## Lexicon Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6RtzBrIll9L",
        "outputId": "eb0400f1-f06c-4106-ccb1-2c8e06b18fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6920 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "# df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "# # only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]\n",
        "\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCvYzNh8mKZV",
        "outputId": "06431c24-13fc-4b6b-83d4-2284d3dfb4ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fungsi untuk mendapatkan skor sentimen VADER dan mengubah label\n",
        "def get_vader_sentiment(text):\n",
        "    sentiment = sid.polarity_scores(text)\n",
        "    # Jika compound > 0, label positive; jika <= 0, label negative\n",
        "    label = 1 if sentiment['compound'] > 0 else 0\n",
        "    return sentiment['neg'], sentiment['neu'], sentiment['pos'], sentiment['compound'], label\n",
        "\n",
        "# Terapkan fungsi ke setiap teks di list\n",
        "lexicon_features = [get_vader_sentiment(text) for text in texts]\n",
        "\n",
        "# Buat DataFrame dari hasil ekstraksi fitur leksikon\n",
        "df_lexicon = pd.DataFrame(lexicon_features, columns=['neg', 'neu', 'pos', 'compound', 'label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGIRtMaJmTWT",
        "outputId": "b83d4968-d826-4e2e-bef3-0cb4ac53882a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13832\n",
            "Found 13831 unique tokens.\n",
            "Shape of data tensor: (6920, 5000)\n",
            "Shape of label tensor: (6920,)\n"
          ]
        }
      ],
      "source": [
        "labels = df_lexicon['label']\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "data = pad_sequences(sequences,\n",
        "                     maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                     padding='pre',\n",
        "                     truncating='pre')\n",
        "print(num_words)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P1hFdnQimW07"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(data,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yGRHV1mYDF",
        "outputId": "fd66a007-49c4-4bb6-88b1-97943109f5a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5000, 64)          885248    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 5000, 128)         66048     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5000, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 993833 (3.79 MB)\n",
            "Trainable params: 993833 (3.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# build a 1D convnet with global maxpooling\n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM,\n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Bidirectional(LSTM(32)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kESS2hD8mZo8",
        "outputId": "ebb0f391-c866-4b9e-92c6-c3812c3661a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "152/152 [==============================] - 147s 892ms/step - loss: 0.6668 - accuracy: 0.5879 - val_loss: 0.5972 - val_accuracy: 0.7057\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 112s 737ms/step - loss: 0.3934 - accuracy: 0.8328 - val_loss: 0.5388 - val_accuracy: 0.7336\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 119s 783ms/step - loss: 0.1516 - accuracy: 0.9523 - val_loss: 0.7308 - val_accuracy: 0.7433\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 109s 718ms/step - loss: 0.0600 - accuracy: 0.9825 - val_loss: 0.9363 - val_accuracy: 0.7307\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 93s 610ms/step - loss: 0.0427 - accuracy: 0.9880 - val_loss: 1.1820 - val_accuracy: 0.7081\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 92s 605ms/step - loss: 0.0329 - accuracy: 0.9895 - val_loss: 1.3270 - val_accuracy: 0.7250\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 91s 602ms/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 1.5487 - val_accuracy: 0.7240\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 91s 601ms/step - loss: 0.0158 - accuracy: 0.9965 - val_loss: 1.4517 - val_accuracy: 0.7283\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 91s 602ms/step - loss: 0.0089 - accuracy: 0.9981 - val_loss: 1.6478 - val_accuracy: 0.7278\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 92s 608ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 1.8560 - val_accuracy: 0.7197\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xsn9YDtma1l",
        "outputId": "2e544887-e248-41f3-d8ab-78e5a3c5e96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 15s 210ms/step\n",
            "Persentase prediksi benar: 51.93%\n"
          ]
        }
      ],
      "source": [
        "# # Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int).flatten()  # Mengonversi probabilitas menjadi label biner dan memastikannya 1D\n",
        "\n",
        "# Ambil indeks dari x_val\n",
        "val_indices = x_val.index if hasattr(x_val, 'index') else range(len(x_val))\n",
        "\n",
        "# Pastikan labels memiliki indeks yang benar\n",
        "labels = df['label']\n",
        "\n",
        "# Buat DataFrame perbandingan\n",
        "comparison_df = pd.DataFrame({'true_label': labels.iloc[val_indices].values, 'pred_label': y_pred})\n",
        "\n",
        "# Menghitung persentase prediksi benar\n",
        "correct_predictions = (comparison_df['true_label'] == comparison_df['pred_label']).sum()\n",
        "total_predictions = len(comparison_df)\n",
        "accuracy = correct_predictions / total_predictions * 100\n",
        "\n",
        "print(f\"Persentase prediksi benar: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3ZPDmbZmgrP"
      },
      "source": [
        "## CV and TF_IDF, Bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2qQhgmmmnrc",
        "outputId": "790b4981-86f0-4c23-9b02-99708e32fd88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6920 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "# df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "# # only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]\n",
        "\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w143sSJ5mq3E"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, max_df=0.95, min_df=2, ngram_range=(1, 2))\n",
        "\n",
        "# Transformasi teks menjadi fitur TF-IDF\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_tfidf,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T_vMCsOm0WS",
        "outputId": "5e19928f-4a23-4764-9f5b-69e99419ee1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5000, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 5000, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 5000, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428585 (1.63 MB)\n",
            "Trainable params: 428585 (1.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "num_words = X_tfidf.shape[1]  # Jumlah kata setara dengan fitur dari TF-IDF\n",
        "\n",
        "# Definisi model LSTM\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM,\n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Bidirectional(LSTM(32)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "499ozKYSm2NK",
        "outputId": "dae4e295-445c-4ee4-8f59-51ca852222b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "152/152 [==============================] - 92s 561ms/step - loss: 0.6937 - accuracy: 0.5109 - val_loss: 0.6930 - val_accuracy: 0.5202\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 90s 593ms/step - loss: 0.6931 - accuracy: 0.5171 - val_loss: 0.6924 - val_accuracy: 0.5202\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 84s 551ms/step - loss: 0.6922 - accuracy: 0.5180 - val_loss: 0.6929 - val_accuracy: 0.5202\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 90s 593ms/step - loss: 0.6924 - accuracy: 0.5231 - val_loss: 0.6925 - val_accuracy: 0.5202\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 90s 595ms/step - loss: 0.6924 - accuracy: 0.5173 - val_loss: 0.6925 - val_accuracy: 0.5202\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 90s 595ms/step - loss: 0.6927 - accuracy: 0.5202 - val_loss: 0.6924 - val_accuracy: 0.5202\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 90s 594ms/step - loss: 0.6927 - accuracy: 0.5163 - val_loss: 0.6926 - val_accuracy: 0.5202\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 90s 594ms/step - loss: 0.6924 - accuracy: 0.5184 - val_loss: 0.6923 - val_accuracy: 0.5202\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 91s 598ms/step - loss: 0.6928 - accuracy: 0.5196 - val_loss: 0.6925 - val_accuracy: 0.5202\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 91s 599ms/step - loss: 0.6921 - accuracy: 0.5190 - val_loss: 0.6924 - val_accuracy: 0.5202\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEfK6vRXm23S",
        "outputId": "f3695644-2279-4902-b490-d592641a16b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 15s 205ms/step\n",
            "Validation Accuracy: 0.5202\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaApKVbem7rA"
      },
      "source": [
        "## CV and TF_IDF, Tri-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaegwuMEm-jt",
        "outputId": "46b61d76-7924-4f71-a2c0-785bc20fc152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6920 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "# df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "# # only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]\n",
        "\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wd6r9JevnAIU"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Inisialisasi TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, max_df=0.95, min_df=2, ngram_range=(1, 3))\n",
        "\n",
        "# Transformasi teks menjadi fitur TF-IDF\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_tfidf,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwX4jFXenB0k",
        "outputId": "7d21dc8c-22ee-498a-9c89-429f36bdf8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 5000, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 5000, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 5000, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428585 (1.63 MB)\n",
            "Trainable params: 428585 (1.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "num_words = X_tfidf.shape[1]  # Jumlah kata setara dengan fitur dari TF-IDF\n",
        "\n",
        "# Definisi model LSTM\n",
        "model = Sequential(\n",
        "    [\n",
        "        # part 1: word and sequence processing\n",
        "        layers.Embedding(num_words,\n",
        "                         EMBEDDING_DIM,\n",
        "                         input_length=MAX_SEQUENCE_LENGTH,\n",
        "                         trainable=True),\n",
        "        layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Bidirectional(LSTM(32)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-IcfRZsnDpk",
        "outputId": "d84dd457-246c-48f8-ebc0-0bd40c35d8eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "152/152 [==============================] - 92s 563ms/step - loss: 0.6941 - accuracy: 0.5039 - val_loss: 0.6918 - val_accuracy: 0.5323\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 90s 596ms/step - loss: 0.6937 - accuracy: 0.5056 - val_loss: 0.6915 - val_accuracy: 0.5323\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 91s 596ms/step - loss: 0.6936 - accuracy: 0.5109 - val_loss: 0.6913 - val_accuracy: 0.5323\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 91s 599ms/step - loss: 0.6928 - accuracy: 0.5147 - val_loss: 0.6911 - val_accuracy: 0.5323\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 91s 601ms/step - loss: 0.6931 - accuracy: 0.5167 - val_loss: 0.6912 - val_accuracy: 0.5323\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 91s 601ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6913 - val_accuracy: 0.5323\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 91s 601ms/step - loss: 0.6928 - accuracy: 0.5184 - val_loss: 0.6919 - val_accuracy: 0.5323\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 91s 599ms/step - loss: 0.6932 - accuracy: 0.5076 - val_loss: 0.6911 - val_accuracy: 0.5323\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 91s 596ms/step - loss: 0.6927 - accuracy: 0.5178 - val_loss: 0.6912 - val_accuracy: 0.5323\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 91s 597ms/step - loss: 0.6929 - accuracy: 0.5171 - val_loss: 0.6917 - val_accuracy: 0.5323\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpA3n84DnFkK",
        "outputId": "1234fa94-3ae7-483e-c929-889ddf735043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 15s 205ms/step\n",
            "Validation Accuracy: 0.5323\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTAFZbbNnKq6"
      },
      "source": [
        "## Word2Vec DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7atPxDV5nTgf",
        "outputId": "eabcee6e-5b44-4adb-c49f-a446aa788859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6920 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "# df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "# # only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]\n",
        "\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LVUomz-nXJc",
        "outputId": "cbbbff03-a654-4df6-800a-660c7f9d121e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jUzLo6oPnfdD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def text_to_vector(text, model):\n",
        "    words = text.split()\n",
        "    word_vectors = [model_w2v[word] for word in words if word in model_w2v]\n",
        "    if len(word_vectors) > 0:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(300)\n",
        "\n",
        "# Transformasi teks menjadi vektor Word2Vec\n",
        "X_word2vec = np.array([text_to_vector(text, model) for text in texts])\n",
        "\n",
        "# Reshape input to include timestep dimension\n",
        "X_word2vec = np.expand_dims(X_word2vec, axis=1)  # Shape will be (samples, 1, 300)\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_word2vec,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rScjKApGnhDL",
        "outputId": "d4b2c370-48fd-43ab-90db-858c281a54aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_6 (Bidirecti  (None, 1, 128)            186880    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 229417 (896.16 KB)\n",
            "Trainable params: 229417 (896.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(1, 300))),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build model\n",
        "model.build(input_shape=(None, 1, 300))\n",
        "\n",
        "# Build model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLSn6mNFniTF",
        "outputId": "601db262-1d5d-42bb-dc8c-c98df5aef5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "152/152 [==============================] - 20s 43ms/step - loss: 0.6093 - accuracy: 0.6691 - val_loss: 0.4463 - val_accuracy: 0.7972\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 4s 25ms/step - loss: 0.4487 - accuracy: 0.7888 - val_loss: 0.4107 - val_accuracy: 0.8194\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 4s 24ms/step - loss: 0.4257 - accuracy: 0.8006 - val_loss: 0.4065 - val_accuracy: 0.8170\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 2s 13ms/step - loss: 0.4118 - accuracy: 0.8072 - val_loss: 0.4349 - val_accuracy: 0.8011\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 2s 12ms/step - loss: 0.4092 - accuracy: 0.8134 - val_loss: 0.4118 - val_accuracy: 0.8174\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 2s 12ms/step - loss: 0.3983 - accuracy: 0.8181 - val_loss: 0.4011 - val_accuracy: 0.8242\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 2s 15ms/step - loss: 0.3936 - accuracy: 0.8210 - val_loss: 0.4023 - val_accuracy: 0.8213\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 2s 15ms/step - loss: 0.3896 - accuracy: 0.8214 - val_loss: 0.4046 - val_accuracy: 0.8237\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 2s 11ms/step - loss: 0.3892 - accuracy: 0.8258 - val_loss: 0.4055 - val_accuracy: 0.8179\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 2s 11ms/step - loss: 0.3818 - accuracy: 0.8229 - val_loss: 0.4075 - val_accuracy: 0.8179\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm4dDiwWnjN1",
        "outputId": "da2ded45-12a4-4f38-9c8b-f2dcec0c35d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 2s 4ms/step\n",
            "Validation Accuracy: 0.8179\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48RoE5Z-noIy"
      },
      "source": [
        "## FastText DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HY5hO61n0Zy",
        "outputId": "e899f22c-2e1b-4980-8570-d2d0b62d19f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6920 texts.\n"
          ]
        }
      ],
      "source": [
        "TEXT_DATA = 'dataset.csv'\n",
        "\n",
        "df = pd.read_csv(TEXT_DATA)\n",
        "# df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "# # only select stories with lengths gt 0 -- there are some texts with len = 0\n",
        "mask = list(df['text'].apply(lambda x: len(x) > 0))\n",
        "df = df[mask]\n",
        "\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# prepare text samples and their labels\n",
        "\n",
        "texts = df['text']\n",
        "labels = df['label']\n",
        "\n",
        "\n",
        "print('Found %s texts.' %texts.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2N9Zf16oDXe",
        "outputId": "5e4238c1-4d04-4827-d84d-9bbadfe00442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the FastText model\n",
        "model_fst = api.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "s11swFkToBwl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def text_to_vector(text, model):\n",
        "    words = text.split()\n",
        "    word_vectors = [model_fst[word] for word in words if word in model_fst]\n",
        "    if len(word_vectors) > 0:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(300)\n",
        "\n",
        "# Transformasi teks menjadi vektor Word2Vec\n",
        "X_fastT = np.array([text_to_vector(text, model_fst) for text in texts])\n",
        "\n",
        "# Reshape input to include timestep dimension\n",
        "X_fastT = np.expand_dims(X_fastT, axis=1)  # Shape will be (samples, 1, 300)\n",
        "\n",
        "# Bagi data menjadi set pelatihan dan validasi\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_fastT,\n",
        "                                                  labels,\n",
        "                                                  test_size=TEST_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-IUdTqoAz9",
        "outputId": "6cd3827a-9b66-4469-a0b7-ecf4c35a05d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_8 (Bidirecti  (None, 1, 128)            186880    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 229417 (896.16 KB)\n",
            "Trainable params: 229417 (896.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True, input_shape=(1, 300))),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build model\n",
        "model.build(input_shape=(None, 1, 300))\n",
        "\n",
        "# Build model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBotqEoEn__J",
        "outputId": "f4a2a878-0e00-4288-a51c-bd96ee18d080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "152/152 [==============================] - 12s 28ms/step - loss: 0.6815 - accuracy: 0.5586 - val_loss: 0.6264 - val_accuracy: 0.6975\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 2s 11ms/step - loss: 0.5175 - accuracy: 0.7457 - val_loss: 0.4795 - val_accuracy: 0.7683\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 2s 12ms/step - loss: 0.4565 - accuracy: 0.7828 - val_loss: 0.4555 - val_accuracy: 0.7905\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 2s 11ms/step - loss: 0.4346 - accuracy: 0.8002 - val_loss: 0.4400 - val_accuracy: 0.7977\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 2s 12ms/step - loss: 0.4233 - accuracy: 0.8051 - val_loss: 0.4439 - val_accuracy: 0.7991\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 2s 11ms/step - loss: 0.4220 - accuracy: 0.8059 - val_loss: 0.4395 - val_accuracy: 0.7943\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 2s 14ms/step - loss: 0.4154 - accuracy: 0.8103 - val_loss: 0.4333 - val_accuracy: 0.8030\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 3s 17ms/step - loss: 0.4082 - accuracy: 0.8117 - val_loss: 0.4516 - val_accuracy: 0.7919\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 2s 12ms/step - loss: 0.4110 - accuracy: 0.8053 - val_loss: 0.4377 - val_accuracy: 0.7924\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 2s 12ms/step - loss: 0.4091 - accuracy: 0.8140 - val_loss: 0.4317 - val_accuracy: 0.8015\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzn_1Hxdn2ap",
        "outputId": "ecbbff89-eb64-4594-859f-398aba2d7b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 3s 6ms/step\n",
            "Validation Accuracy: 0.8015\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(x_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Mengonversi probabilitas menjadi label biner\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YFPHg-0NBBso",
        "lExjL2PbMStw",
        "BmrYS6TY-Gd5",
        "6dOgxNOm6K1b",
        "GY-wM9Wdrb0z",
        "kMzl-kxJlk69",
        "p3ZPDmbZmgrP",
        "YaApKVbem7rA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
